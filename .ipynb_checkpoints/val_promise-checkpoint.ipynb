{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a27a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import monai\n",
    "from local_segment_anything import SamPredictor, sam_model_registry\n",
    "from local_segment_anything.utils.transforms import ResizeLongestSide\n",
    "import argparse\n",
    "# set seeds\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)\n",
    "#self dataset\n",
    "from datasets.self_dataset import PolypDataset\n",
    "from utils.tools import compute_num_params\n",
    "from utils.tools import set_save_path\n",
    "from utils.tools import calc_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2be3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_from_mask(mask):\n",
    "    '''Returns a bounding box from a mask'''\n",
    "    y_indices, x_indices = np.where(mask > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    # add perturbation to bounding box coordinates\n",
    "    H, W = mask.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "\n",
    "    return np.array([x_min, y_min, x_max, y_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9227c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vit_b'\n",
    "image_root = '/home/hjh/data_disk/hjh/dataset/Kvasir-SEG/images'\n",
    "mask_root = '/home/hjh/data_disk/hjh/dataset/Kvasir-SEG/masks_new'\n",
    "separate_info = '/home/hjh/data_disk/hjh/dataset/Kvasir-SEG/fold_info/kvasir_fold_0.csv'\n",
    "ori_model_path = '/home/hjh/code_lib/SAM/sam_vit_b_01ec64.pth'\n",
    "# finetune_model_path = ''\n",
    "device = 'cuda:0'\n",
    "sam_model_ori = sam_model_registry[model_name](checkpoint=ori_model_path).to(device)\n",
    "sam_model =sam_model_ori\n",
    "# sam_model_fine = sam_model_registry[model_type](checkpoint=finetune_model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3dca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dataset is 200\n",
      "[Resize(always_apply=False, p=1.0, height=1024, width=1024, interpolation=1), Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255)]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = PolypDataset(image_root=image_root, gt_root=mask_root, separate_info=separate_info, train_flag=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4d7b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:46<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation results: SM(0.9133) EM(0.9017) mDice(89.82) mIoU(83.68)\n"
     ]
    }
   ],
   "source": [
    "import medpy.metric.binary as eval_tool\n",
    "sam_model.eval()\n",
    "pred_list = []\n",
    "gt_list = []\n",
    "epoch = 0\n",
    "for step, (image, gt2D, boxes) in enumerate(tqdm(valid_dataloader, desc='valid')):\n",
    "     # do not compute gradients for image encoder and prompt encoder\n",
    "    with torch.no_grad():\n",
    "        image_embedding = sam_model.image_encoder(image.to(device))\n",
    "\n",
    "        # convert box to 1024x1024 grid\n",
    "#         box_np = boxes.numpy()\n",
    "#         sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "#         box = sam_trans.apply_boxes(box_np, (gt2D.shape[-2], gt2D.shape[-1]))\n",
    "#         box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "        box_torch = boxes.to(device)\n",
    "        if len(box_torch.shape) == 2:\n",
    "            box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=box_torch,\n",
    "            masks=None,\n",
    "        )\n",
    "        low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "            )\n",
    "        pred_list.append(torch.sigmoid(low_res_masks))\n",
    "        gt_list.append(gt2D)\n",
    "\n",
    "pred_list = torch.cat(pred_list, 0)\n",
    "gt_list = torch.cat(gt_list, 0)\n",
    "\n",
    "sm, em, mDice, mIoU = calc_seg(pred_list, gt_list)\n",
    "print('epoch %d validation results: SM(%.4f) EM(%.4f) mDice(%.2f) mIoU(%.2f)'%(epoch, sm, em, mDice*100, mIoU*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda4e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e765c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, para in sam_model.named_parameters():\n",
    "    if 'mask_decoder' in name:\n",
    "        para.requires_grad_(True)\n",
    "    elif 'prompt_encoder' in name:\n",
    "        para.requires_grad_(False)\n",
    "    elif \"image_encoder\" in name and (\"interaction\" not in name and 'SPM' not in name):\n",
    "        para.requires_grad_(False)\n",
    "    else:\n",
    "        para.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137dee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, para in sam_model.named_parameters():\n",
    "    if para.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.randint(10, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd89289",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3,4][np.random.randint(3, size=1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ae0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c05f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ebc951",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.zeros((100, 200))\n",
    "test_image[20:50, 70:100] = 1\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738da42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
